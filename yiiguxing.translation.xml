<application>
  <component name="AppStorage">
    <histories>
      <item value="Get the millis of day property which provides access to advanced functionality." />
      <item value="Build a step that processes items in chunks with the size provided" />
      <item value="numeric field" />
      <item value="字段" />
      <item value="统计分析" />
      <item value="景区" />
      <item value="times" />
      <item value="Pushes an element onto the stack represented by this list." />
      <item value="transient" />
      <item value="Random Access" />
      <item value="More formally" />
      <item value="The maximum size of array to allocate." />
      <item value="ensure Capacity" />
      <item value="Trims the capacity of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance to be the * list's current size." />
      <item value="Constructs an empty list with an initial capacity of ten." />
      <item value="the initial capacity of the list" />
      <item value="The size of the ArrayList (the number of elements it contains)." />
      <item value="We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added." />
      <item value="Shared empty array instance used for default sized empty instances." />
      <item value="The array buffer into which the elements of the ArrayList are stored." />
      <item value="Shared empty array instance used for empty instances." />
      <item value="EMPTY ELEMENTDATA" />
      <item value="Default initial capacity." />
      <item value="deallocated" />
      <item value="Decreases the reference count by {@code 1} and deallocates this object if the reference count reaches at" />
      <item value="mplement the methods of this class." />
      <item value="is the root of the class hierarchy." />
      <item value="Adds a number of elements provided by a traversable object * and returns a new collection with the added elements." />
      <item value="Traversable Like" />
      <item value="When a Spark Streaming job recovers from checkpoint, this exception will be hit if a reference to an RDD not defined by the streaming job is used in DStream operations. For more information" />
      <item value="RDD transformations and actions are NOT invoked by the driver, but inside of other transformations; for example, rdd1.map(x =&gt; rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063." />
      <item value="the transformation function mapping elements * to some other domain `B`" />
      <item value="assumed" />
      <item value="chgrp" />
      <item value="expunge" />
      <item value="Minimum number of Hadoop Splits to generate" />
      <item value="return type of UDF" />
      <item value="Register a Scala closure of 0 arguments as user-defined function (UDF)." />
      <item value="A closure in Scala" />
      <item value="随机" />
      <item value="Returns a sort expression based on the descending order of the column" />
      <item value="literal value" />
      <item value="argument 2 requires integral type" />
      <item value="integral" />
      <item value="Division this expression by another expression" />
      <item value="Identifies a type that the injector only instantiates once. Not inherited." />
      <item value="Only works if the source is a HadoopFsRelationProvider." />
      <item value="Loads input in as a `DataFrame`, for data sources that support multiple paths." />
      <item value="Specifies the input data source format" />
      <item value="duration" />
    </histories>
  </component>
  <component name="Settings">
    <option name="googleTranslateSettings">
      <google-translate>
        <option name="primaryLanguage" value="CHINESE" />
      </google-translate>
    </option>
    <option name="overrideFont" value="true" />
  </component>
</application>